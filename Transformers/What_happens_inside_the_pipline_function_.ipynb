{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PriqNlqZ9ew0",
        "outputId": "5b932e45-7ca9-4535-d08d-10d4f0d3103d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "KGAsfsnR9iiH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HUgging face course my whoel life!\",\n",
        "    \"I hate this so much\"\n",
        "]\n",
        "\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "#The padding function is required every time that the sentences given to the tokenizer are shorter than the maximum allowed by the model\n",
        "#The issue but with the sentences longer than the maximum of the model goes with the parameter truncation"
      ],
      "metadata": {
        "id": "11Nw1HDN95Q7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs\n",
        "#The two dictionaries shown show the conversion to a vector form the original words and the attention_mask includes the parts where padding has been applied so the model didn't pay attention to it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBXJLUMi-U-k",
        "outputId": "88455ca7-93d3-4854-8949-4215fb5edf18"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,  2227,\n",
              "          2607,  2026,  2040,  2884,  2166,   999,   102],\n",
              "        [  101,  1045,  5223,  2023,  2061,  2172,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that we know how the toknizer works let's take a look at the model\n",
        "\n",
        "from transformers import AutoModel\n",
        "\n",
        "#THe AutoModel from pretrained calls an api that returns the core of the model leaving out the head or the fine tuned final part of the model\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "#THe output will be a high dimensional tensro that is the representation of the sentences passed to the model\n",
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)\n",
        "\n",
        "###torch.Size([2, 17, 768])\n",
        "\n",
        "#In this case we can see that we have 2 sentences of 17 tokens (including special tokens) and a hidden state of 768\n",
        "#Though this is a first step to our classification problem we need to do one more thing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ACaxfVy__5n",
        "outputId": "0187b4ef-c4d8-48b0-e4cd-12b1f85fa093"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 17, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "#Now that we've used a different AutoModel specifically for a classification task (it is said head of the model so it is trained to perform this task) we can use the logits in order to fullfil our classification problem\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "print(outputs.logits)\n",
        "\n",
        "#The result is\n",
        "\n",
        "#tensor([[-3.2224,  3.4311],\n",
        "#        [ 4.2141, -3.4158]], grad_fn=<AddmmBackward0>)\n",
        "\n",
        "#The matrix has a vector for each sentences and for each possible label."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGOlBAH4BTRB",
        "outputId": "f08d9e04-b937-453c-b24d-c2279dd2fd61"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-3.2224,  3.4311],\n",
            "        [ 4.2141, -3.4158]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can see that the logits are not probabilities, to make sense of the logits we need to move forward to the next step: Postprocessing"
      ],
      "metadata": {
        "id": "Rx_ncPa8CU8a"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5qocjTnClAe",
        "outputId": "37f7491a-e87b-4823-f5a4-04524893cbbc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)\n",
        "\n",
        "#THe softmax function only converts the logits to a normal distribution so it can then be used to show probabilities between 0 an 1\n",
        "#The output is\n",
        "\n",
        "#tensor([[1.2878e-03, 9.9871e-01],\n",
        "#        [9.9951e-01, 4.8549e-04]], grad_fn=<SoftmaxBackward0>)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdqgGIgdD7AP",
        "outputId": "6e631bd0-e112-4a38-b49f-e1d352b1bbf6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2878e-03, 9.9871e-01],\n",
            "        [9.9951e-01, 4.8549e-04]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#THe final step is then check which correspond to the positivity or negativity label. This is given by the id2label field of the model configuration. In our case we would have somthing like this:\n",
        "model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afJlN_FyEeQu",
        "outputId": "ea5f707f-1eeb-418d-c673-cea6839f46cc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#So seeing our results the values out come would be something like:\n",
        "#-first sentence: NEGATIVE 0.001287 -> 0.2% POSITIVE 0.998 -> 99.8%\n",
        "#-second sentence: NEGATIVE 0.9995 -> 99.95% POSITIVE 0.00048 -> 0.48%"
      ],
      "metadata": {
        "id": "z3lrj0AoFCof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}